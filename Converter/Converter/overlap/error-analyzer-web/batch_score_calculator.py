import os
import sys
from pathlib import Path
from calculate_slide_score import calculate_slide_error_score
import json


def find_slide_pairs(folder_path):
    """
    TÃ¬m táº¥t cáº£ cÃ¡c cáº·p slide (input.html, output.html) trong cÃ¡c subfolder.
    
    Cáº¥u trÃºc yÃªu cáº§u:
    slides_folder/
      â”œâ”€â”€ slide_01/
      â”‚   â”œâ”€â”€ input.html
      â”‚   â””â”€â”€ output.html
      â”œâ”€â”€ slide_02/
          â”œâ”€â”€ input.html
          â””â”€â”€ output.html
    
    Returns:
        list: [(input_path, output_path, slide_name), ...]
    """
    folder = Path(folder_path)
    
    if not folder.exists():
        raise FileNotFoundError(f"Folder not found: {folder}")
    
    pairs = []
    
    # Duyá»‡t qua tá»«ng subfolder
    for subfolder in sorted([f for f in folder.iterdir() if f.is_dir()]):
        input_file = subfolder / "input.html"
        output_file = subfolder / "output.html"
        
        if input_file.exists() and output_file.exists():
            pairs.append((input_file, output_file, subfolder.name))
        else:
            # Cáº£nh bÃ¡o náº¿u thiáº¿u file
            if not input_file.exists():
                print(f"   âš ï¸  Missing input.html in {subfolder.name}")
            if not output_file.exists():
                print(f"   âš ï¸  Missing output.html in {subfolder.name}")
    
    return pairs


def batch_calculate_scores(folder_path, output_json=None, verbose=False):
    """
    TÃ­nh Ä‘iá»ƒm cho táº¥t cáº£ cÃ¡c slide trong folder.
    
    Args:
        folder_path: ÄÆ°á»ng dáº«n Ä‘áº¿n folder chá»©a slides
        output_json: ÄÆ°á»ng dáº«n file JSON Ä‘á»ƒ lÆ°u káº¿t quáº£ (optional)
        verbose: In chi tiáº¿t tá»«ng slide
        
    Returns:
        dict: {
            'total_slides': int,
            'average_score': float,
            'slides': [
                {
                    'name': str,
                    'input_file': str,
                    'output_file': str,
                    'score': float,
                    'total_errors': int,
                    'overlap_errors': int,
                    'container_overflow_errors': int,
                    'viewport_overflow_errors': int,
                    'error_percentages': list
                },
                ...
            ],
            'summary': {
                'excellent': int,  # score >= 9.0
                'good': int,       # score >= 7.5
                'fair': int,       # score >= 5.0
                'poor': int,       # score >= 2.5
                'critical': int    # score < 2.5
            }
        }
    """
    print(f"ğŸ” Scanning folder: {folder_path}\n")
    
    pairs = find_slide_pairs(folder_path)
    
    if not pairs:
        print("âŒ No slide pairs found!")
        print("\nExpected structure:")
        print("  slides_folder/")
        print("    â”œâ”€â”€ slide_01/")
        print("    â”‚   â”œâ”€â”€ input.html")
        print("    â”‚   â””â”€â”€ output.html")
        print("    â”œâ”€â”€ slide_02/")
        print("        â”œâ”€â”€ input.html")
        print("        â””â”€â”€ output.html")
        return None
    
    print(f"âœ… Found {len(pairs)} slide pairs\n")
    print("="*80)
    
    results = {
        'total_slides': len(pairs),
        'average_score': 0.0,
        'slides': [],
        'summary': {
            'excellent': 0,
            'good': 0,
            'fair': 0,
            'poor': 0,
            'critical': 0
        }
    }
    
    total_score = 0.0
    
    for idx, (input_file, output_file, slide_name) in enumerate(pairs, 1):
        print(f"\n[{idx}/{len(pairs)}] Processing: {slide_name}")
        print(f"   Input:  {input_file.name}")
        print(f"   Output: {output_file.name}")
        
        try:
            score_result = calculate_slide_error_score(str(input_file), str(output_file))
            
            slide_data = {
                'name': slide_name,
                'input_file': str(input_file),
                'output_file': str(output_file),
                'score': score_result['score'],
                'total_errors': score_result['total_errors'],
                'overlap_errors': score_result['overlap_errors'],
                'container_overflow_errors': score_result['container_overflow_errors'],
                'viewport_overflow_errors': score_result['viewport_overflow_errors'],
                'error_percentages': score_result['error_percentages']
            }
            
            results['slides'].append(slide_data)
            total_score += score_result['score']
            
            # PhÃ¢n loáº¡i cháº¥t lÆ°á»£ng
            score = score_result['score']
            if score >= 9.0:
                quality = "âœ… EXCELLENT"
                results['summary']['excellent'] += 1
            elif score >= 7.5:
                quality = "ğŸŸ¢ GOOD"
                results['summary']['good'] += 1
            elif score >= 5.0:
                quality = "ğŸŸ¡ FAIR"
                results['summary']['fair'] += 1
            elif score >= 2.5:
                quality = "ğŸŸ  POOR"
                results['summary']['poor'] += 1
            else:
                quality = "ğŸ”´ CRITICAL"
                results['summary']['critical'] += 1
            
            print(f"   Score: {score:.2f} / 10.00 ({quality})")
            print(f"   Errors: {score_result['total_errors']} total "
                  f"({score_result['overlap_errors']} overlap, "
                  f"{score_result['container_overflow_errors']} container, "
                  f"{score_result['viewport_overflow_errors']} viewport)")
            
            if verbose and score_result['total_errors'] > 0:
                _print_brief_errors(score_result)
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
            results['slides'].append({
                'name': slide_name,
                'input_file': str(input_file),
                'output_file': str(output_file),
                'score': 0.0,
                'total_errors': -1,
                'error': str(e)
            })
    
    # TÃ­nh Ä‘iá»ƒm trung bÃ¬nh
    if results['slides']:
        valid_scores = [s['score'] for s in results['slides'] if s.get('score', 0) > 0]
        results['average_score'] = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
    
    # In bÃ¡o cÃ¡o tá»•ng há»£p
    _print_batch_summary(results)
    
    # LÆ°u JSON náº¿u Ä‘Æ°á»£c yÃªu cáº§u
    if output_json:
        output_path = Path(output_json)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ Results saved to: {output_path}")
    
    return results


def _print_brief_errors(score_result):
    """In tÃ³m táº¯t cÃ¡c lá»—i"""
    details = score_result['details']
    
    if details['overlap']:
        print(f"      Overlaps:")
        for err in details['overlap'][:3]:  # Chá»‰ in 3 lá»—i Ä‘áº§u
            print(f"        â€¢ {err['overlap_percent']:.1f}% - "
                  f"{err['element1']['tag']}.{err['element1']['class']} â†” "
                  f"{err['element2']['tag']}.{err['element2']['class']}")
        if len(details['overlap']) > 3:
            print(f"        ... and {len(details['overlap']) - 3} more")
    
    if details['container_overflow']:
        print(f"      Container overflows:")
        for err in details['container_overflow'][:3]:
            print(f"        â€¢ {err['overflow_percent']:.1f}% - "
                  f"{err['text']['tag']}.{err['text']['class']} in "
                  f"{err['parent']['tag']}.{err['parent']['class']}")
        if len(details['container_overflow']) > 3:
            print(f"        ... and {len(details['container_overflow']) - 3} more")
    
    if details['viewport_overflow']:
        print(f"      Viewport overflows:")
        for err in details['viewport_overflow'][:3]:
            directions = err['overflow']['directions']
            print(f"        â€¢ {err['overflow_percent']:.1f}% - "
                  f"{err['text']['tag']}.{err['text']['class']} "
                  f"({', '.join(directions)})")
        if len(details['viewport_overflow']) > 3:
            print(f"        ... and {len(details['viewport_overflow']) - 3} more")


def _print_batch_summary(results):
    """In bÃ¡o cÃ¡o tá»•ng há»£p"""
    print("\n" + "="*80)
    print("ğŸ“Š BATCH SCORING SUMMARY")
    print("="*80)
    
    print(f"\nğŸ“ Total Slides: {results['total_slides']}")
    print(f"ğŸ“ˆ Average Score: {results['average_score']:.2f} / 10.00")
    
    print("\nğŸ¯ Quality Distribution:")
    print(f"   âœ… EXCELLENT (â‰¥9.0):  {results['summary']['excellent']:3d} slides")
    print(f"   ğŸŸ¢ GOOD (â‰¥7.5):       {results['summary']['good']:3d} slides")
    print(f"   ğŸŸ¡ FAIR (â‰¥5.0):       {results['summary']['fair']:3d} slides")
    print(f"   ğŸŸ  POOR (â‰¥2.5):       {results['summary']['poor']:3d} slides")
    print(f"   ğŸ”´ CRITICAL (<2.5):   {results['summary']['critical']:3d} slides")
    
    # Top 5 best slides
    sorted_slides = sorted(results['slides'], key=lambda x: x.get('score', 0), reverse=True)
    
    print("\nğŸ† Top 5 Best Slides:")
    for idx, slide in enumerate(sorted_slides[:5], 1):
        print(f"   {idx}. {slide['name']:30s} - {slide['score']:.2f} / 10.00")
    
    # Top 5 worst slides
    print("\nâš ï¸  Top 5 Worst Slides:")
    for idx, slide in enumerate(sorted_slides[-5:][::-1], 1):
        print(f"   {idx}. {slide['name']:30s} - {slide['score']:.2f} / 10.00")
    
    # Tá»•ng sá»‘ lá»—i
    total_errors = sum(s.get('total_errors', 0) for s in results['slides'] if s.get('total_errors', 0) > 0)
    total_overlap = sum(s.get('overlap_errors', 0) for s in results['slides'])
    total_container = sum(s.get('container_overflow_errors', 0) for s in results['slides'])
    total_viewport = sum(s.get('viewport_overflow_errors', 0) for s in results['slides'])
    
    print(f"\nğŸ“Š Total Errors Detected: {total_errors}")
    print(f"   - Text overlaps:          {total_overlap}")
    print(f"   - Container overflows:    {total_container}")
    print(f"   - Viewport overflows:     {total_viewport}")
    
    print("\n" + "="*80)


# Main function
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Batch calculate quality scores for multiple slides",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Expected folder structure:
  slides_folder/
    â”œâ”€â”€ slide_01/
    â”‚   â”œâ”€â”€ input.html
    â”‚   â””â”€â”€ output.html
    â”œâ”€â”€ slide_02/
        â”œâ”€â”€ input.html
        â””â”€â”€ output.html

Examples:
  python batch_score_calculator.py slides_folder
  python batch_score_calculator.py slides_folder --output results.json
  python batch_score_calculator.py slides_folder --verbose
        """
    )
    
    parser.add_argument('folder', help='Folder containing slide pairs (input.html / output.html)')
    parser.add_argument('--output', '-o', help='Save results to JSON file')
    parser.add_argument('--verbose', '-v', action='store_true', help='Print detailed errors for each slide')
    
    args = parser.parse_args()
    
    try:
        results = batch_calculate_scores(args.folder, args.output, args.verbose)
        
        if results:
            # Exit code: 0 náº¿u average score >= 5.0, 1 náº¿u < 5.0
            sys.exit(0 if results['average_score'] >= 5.0 else 1)
        else:
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
